{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting Rouge\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from Rouge) (1.16.0)\n",
      "Installing collected packages: Rouge\n",
      "Successfully installed Rouge-1.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install Rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ROUGE-1 Precision: 0.6228\n",
      "Average ROUGE-1 Recall: 0.1322\n",
      "Average ROUGE-1 F1 Score: 0.2083\n",
      "\n",
      "Average ROUGE-2 Precision: 0.3264\n",
      "Average ROUGE-2 Recall: 0.0544\n",
      "Average ROUGE-2 F1 Score: 0.0888\n",
      "\n",
      "Average ROUGE-L Precision: 0.5856\n",
      "Average ROUGE-L Recall: 0.1230\n",
      "Average ROUGE-L F1 Score: 0.1942\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from rouge import Rouge\n",
    "\n",
    "\n",
    "rouge = Rouge()\n",
    "\n",
    "\n",
    "llm_folder = \"LLM_generated_summary\"\n",
    "reference_folder = \"test_sum\"\n",
    "\n",
    "\n",
    "precision_rouge_1 = []\n",
    "recall_rouge_1 = []\n",
    "f1_score_rouge_1 = []\n",
    "precision_rouge_2 = []\n",
    "recall_rouge_2 = []\n",
    "f1_score_rouge_2 = []\n",
    "precision_rouge_l = []\n",
    "recall_rouge_l = []\n",
    "f1_score_rouge_l = []\n",
    "\n",
    "\n",
    "for llm_file in os.listdir(llm_folder):\n",
    "    if not llm_file.endswith(\".txt\"):\n",
    "        continue\n",
    "    reference_file = llm_file.replace(\".txt\", \".txt\")\n",
    "\n",
    "  \n",
    "    llm_file_path = os.path.join(llm_folder, llm_file)\n",
    "    with open(llm_file_path, \"r\") as f:\n",
    "        llm_summary = f.read()\n",
    "\n",
    "    reference_file_path = os.path.join(reference_folder, reference_file)\n",
    "    with open(reference_file_path, \"r\") as f:\n",
    "        reference_summary = f.read()\n",
    "\n",
    "  \n",
    "    scores = rouge.get_scores(llm_summary, reference_summary)\n",
    "\n",
    "    \n",
    "    rouge_1 = scores[0]['rouge-1']\n",
    "    rouge_2 = scores[0]['rouge-2']\n",
    "    rouge_l = scores[0]['rouge-l']\n",
    "\n",
    "    \n",
    "    rouge_1_precision = rouge_1['p']\n",
    "    rouge_1_recall = rouge_1['r']\n",
    "    rouge_1_f1_score = rouge_1['f']\n",
    "\n",
    "\n",
    "    rouge_2_precision = rouge_2['p']\n",
    "    rouge_2_recall = rouge_2['r']\n",
    "    rouge_2_f1_score = rouge_2['f']\n",
    "\n",
    "  \n",
    "    rouge_l_precision = rouge_l['p']\n",
    "    rouge_l_recall = rouge_l['r']\n",
    "    rouge_l_f1_score = rouge_l['f']\n",
    "\n",
    "\n",
    "    precision_rouge_1.append(rouge_1_precision)\n",
    "    recall_rouge_1.append(rouge_1_recall)\n",
    "    f1_score_rouge_1.append(rouge_1_f1_score)\n",
    "\n",
    "  \n",
    "    precision_rouge_2.append(rouge_2_precision)\n",
    "    recall_rouge_2.append(rouge_2_recall)\n",
    "    f1_score_rouge_2.append(rouge_2_f1_score)\n",
    "\n",
    "   \n",
    "    precision_rouge_l.append(rouge_l_precision)\n",
    "    recall_rouge_l.append(rouge_l_recall)\n",
    "    f1_score_rouge_l.append(rouge_l_f1_score)\n",
    "\n",
    "\n",
    "avg_precision_rouge_1 = sum(precision_rouge_1) / len(precision_rouge_1)\n",
    "avg_recall_rouge_1 = sum(recall_rouge_1) / len(recall_rouge_1)\n",
    "avg_f1_score_rouge_1 = sum(f1_score_rouge_1) / len(f1_score_rouge_1)\n",
    "\n",
    "\n",
    "avg_precision_rouge_2 = sum(precision_rouge_2) / len(precision_rouge_2)\n",
    "avg_recall_rouge_2 = sum(recall_rouge_2) / len(recall_rouge_2)\n",
    "avg_f1_score_rouge_2 = sum(f1_score_rouge_2) / len(f1_score_rouge_2)\n",
    "\n",
    "avg_precision_rouge_l = sum(precision_rouge_l) / len(precision_rouge_l)\n",
    "avg_recall_rouge_l = sum(recall_rouge_l) / len(recall_rouge_l)\n",
    "avg_f1_score_rouge_l = sum(f1_score_rouge_l) / len(f1_score_rouge_l)\n",
    "\n",
    "\n",
    "print(\"Average ROUGE-1 Precision: {:.4f}\".format(avg_precision_rouge_1))\n",
    "print(\"Average ROUGE-1 Recall: {:.4f}\".format(avg_recall_rouge_1))\n",
    "print(\"Average ROUGE-1 F1 Score: {:.4f}\".format(avg_f1_score_rouge_1))\n",
    "print()\n",
    "print(\"Average ROUGE-2 Precision: {:.4f}\".format(avg_precision_rouge_2))\n",
    "print(\"Average ROUGE-2 Recall: {:.4f}\".format(avg_recall_rouge_2))\n",
    "print(\"Average ROUGE-2 F1 Score: {:.4f}\".format(avg_f1_score_rouge_2))\n",
    "print()\n",
    "print(\"Average ROUGE-L Precision: {:.4f}\".format(avg_precision_rouge_l))\n",
    "print(\"Average ROUGE-L Recall: {:.4f}\".format(avg_recall_rouge_l))\n",
    "print(\"Average ROUGE-L F1 Score: {:.4f}\".format(avg_f1_score_rouge_l))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
